{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084a5836-46c8-47ac-a1e0-e8348a525150",
   "metadata": {},
   "source": [
    "# ProgrammingAssignment06_clusterAnalysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bf8149-9aa3-4627-8f2c-49e531f9c5e9",
   "metadata": {},
   "source": [
    "## 1. k-means using scikit-learn\n",
    "The healthy_lifestyle dataset contains information on lifestyle measures such as amount of sunshine, pollution, and happiness levels for 44 major cities around the world. Apply k-means clustering to the cities' number of hours of sunshine and happiness levels.\n",
    "\n",
    "- Import the needed packages for clustering.\n",
    "- Initialize and fit a k-means clustering model using sklearn's Kmeans() function.\n",
    "- Use the user-defined number of clusters, init='random', n_init=10, random_state=123, and algorithm='elkan'.\n",
    "- Find the cluster centroids and inertia.\n",
    "\n",
    "Ex: If the input is: 4\n",
    "\n",
    "the output should be:\n",
    "\n",
    "- Centroids: [[ 0.8294  0.2562]\n",
    " [ 1.3106 -1.887 ]\n",
    " [-0.9471  0.8281]\n",
    " [-0.6372 -0.7943]]\n",
    "- Inertia: 16.4991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad54189b-81b1-4a58-9dd8-42ca5ba05310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroids: [[-0.9471  0.8281]\n",
      " [ 0.8294  0.2562]\n",
      " [ 1.3106 -1.887 ]\n",
      " [-0.6372 -0.7943]]\n",
      "Inertia: 16.4991\n"
     ]
    }
   ],
   "source": [
    "# Import needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Your code here\n",
    "\n",
    "healthy = pd.read_csv('healthy_lifestyle.csv')\n",
    "\n",
    "# Input the number of clusters\n",
    "number = int(input())\n",
    "\n",
    "# Define input features\n",
    "X = healthy[['sunshine_hours', 'happiness_levels']]\n",
    "\n",
    "# Use StandardScaler() to standardize input features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X, columns=['sunshine_hours', 'happiness_levels'])\n",
    "X = X.dropna()\n",
    "\n",
    "# Initialize a k-means clustering algorithm with a user-defined number of clusters, init='random', n_init=10, \n",
    "# random_state=123, and algorithm='elkan'\n",
    "# Your code here\n",
    "kmeans = KMeans(n_clusters=4, init='random', n_init=10, random_state=123, algorithm='elkan')\n",
    "\n",
    "# Fit the algorithm to the input features\n",
    "# Your code here\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Find and print the cluster centroids\n",
    "centroid = kmeans.cluster_centers_ # Your code here\n",
    "print(\"Centroids:\", np.round(centroid,4))\n",
    "\n",
    "# Find and print the cluster inertia\n",
    "inertia = kmeans.inertia_ # Your code here\n",
    "print(\"Inertia:\", np.round(inertia,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18700cd6-8069-458a-8822-61f755fd893e",
   "metadata": {},
   "source": [
    "## 2. Hierarchical clustering using scikit-learn\n",
    "The healthy_lifestyle dataset contains information on lifestyle measures such as amount of sunshine, pollution, and happiness levels for 44 major cities around the world. Apply agglomerative clustering to the cities' number of hours of sunshine and happiness levels using both sklearn and SciPy.\n",
    "\n",
    "- Import the needed packages for agglomerative clustering from sklearn and SciPy.\n",
    "- Initialize and fit an agglomerative clustering model using sklearn's AgglomerativeClustering() function. Use the user-defined number of clusters and ward linkage.\n",
    "- Add cluster labels to the input feature dataframe.\n",
    "- Calculate the distances between all instances using SciPy's pdist() function.\n",
    "- Convert the distance matrix to a square matrix using SciPy's squareform() function.\n",
    "- Define a clustering model with ward linkage using SciPy's linkage() function.\n",
    "\n",
    "Ex: If the input is: 4\n",
    "\n",
    "the output should be:\n",
    "|       | sunshine_hours | happiness_levels | labels |\n",
    "|-------|----------------|------------------|--------|\n",
    "| 0     | -0.691660      | 1.025642         | 3      |\n",
    "| 1     | 0.695725       | 0.801124         | 0      |\n",
    "| 2     | -0.645295      | 0.872562         | 3      |\n",
    "| 3     | -0.757641      | 0.933794         | 3      |\n",
    "| 4     | -1.098246      | 1.229750         | 3      |\n",
    "\n",
    "\n",
    "First five rows of the linkage matrix from SciPy:\n",
    "    \n",
    " - [[39. 40.  0.  2.]\n",
    " [28. 43.  0.  3.]\n",
    " [ 7. 18.  0.  2.]\n",
    " [ 8. 42.  0.  2.]\n",
    " [ 0.  3.  0.  2.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f58d1471-d33e-43b5-839c-ba86c9509ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sunshine_hours  happiness_levels  labels\n",
      "0       -0.691660          1.025642       3\n",
      "1        0.695725          0.801124       0\n",
      "2       -0.645295          0.872562       3\n",
      "3       -0.757641          0.933794       3\n",
      "4       -1.098246          1.229750       3\n",
      "First five rows of the linkage matrix from SciPy:\n",
      " [[39. 40.  0.  2.]\n",
      " [28. 43.  0.  3.]\n",
      " [ 7. 18.  0.  2.]\n",
      " [ 8. 42.  0.  2.]\n",
      " [36. 45.  0.  3.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import needed sklearn packages\n",
    "# Your code here\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import needed scipy packages\n",
    "# Your code here\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "# Silence warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "healthy = pd.read_csv('healthy_lifestyle.csv')\n",
    "\n",
    "# Input the number of clusters\n",
    "number = int(input())\n",
    "\n",
    "# Define input features\n",
    "X = healthy[['sunshine_hours', 'happiness_levels']]\n",
    "\n",
    "# Use StandardScaler() to standardize input features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X, columns=['sunshine_hours', 'happiness_levels'])\n",
    "X = X.dropna()\n",
    "\n",
    "# Initialize and fit an agglomerative clustering model using ward linkage in scikit-learn, with a user-defined\n",
    "# number of clusters\n",
    "# Your code here\n",
    "n_clusters = 4\n",
    "agg_clust = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "\n",
    "# Add cluster labels to input feature dataframe\n",
    "X['labels']= agg_clust.fit_predict(X[['sunshine_hours', 'happiness_levels']]) # Your code here\n",
    "print(X.head())\n",
    "\n",
    "# Perform agglomerative clustering using SciPy\n",
    "\n",
    "# Calculate the distances between all instances\n",
    "# Your code here\n",
    "distance_matrix = pdist(X[['sunshine_hours', 'happiness_levels']], metric='euclidean')\n",
    "\n",
    "# Convert the distance matrix to a square matrix\n",
    "# Your code here\n",
    "square_distance_matrix = squareform(distance_matrix)\n",
    "\n",
    "# Define a clustering model with ward linkage\n",
    "clustersHealthyScipy = linkage(X[['sunshine_hours', 'happiness_levels']], method='ward') # Your code here\n",
    "\n",
    "print('First five rows of the linkage matrix from SciPy:\\n', np.round(clustersHealthyScipy[:5, :], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db51dd-7ad3-4f58-b0ff-ae71a6c306e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d92b04d3-05bd-470c-a199-327a6b55679c",
   "metadata": {},
   "source": [
    "## 3. DBSCAN using scikit-learn\n",
    "- Increase the **number of points sampled to 500**.\n",
    "- Apply the DBSCAN model with **epsilon=1** and **min_samples=8** to identify the number of core-points and outliers (or noise). \n",
    "- EX: if the epsilon=1 and min_samples = 10 and number of points sampled to 100.\n",
    "  - the number of core-points = 85\n",
    "  - the number of outliers    = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdeae7b7-d4e7-483a-81bc-1d4bd4d6716c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DBSCAN\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load the full grocery customer dataset and take a random sample of 500 instances\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_personality.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msample(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m2\u001b[39m)) \u001b[38;5;66;03m# your code here , random_state=123)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Use StandardScaler() to standardize input features\u001b[39;00m\n\u001b[0;32m     13\u001b[0m X \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFruits\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMeats\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6110\u001b[0m, in \u001b[0;36mNDFrame.sample\u001b[1;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[0;32m   6107\u001b[0m \u001b[38;5;66;03m# Process random_state argument\u001b[39;00m\n\u001b[0;32m   6108\u001b[0m rs \u001b[38;5;241m=\u001b[39m common\u001b[38;5;241m.\u001b[39mrandom_state(random_state)\n\u001b[1;32m-> 6110\u001b[0m size \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mprocess_sampling_size(n, frac, replace)\n\u001b[0;32m   6111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   6112\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m frac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\sample.py:96\u001b[0m, in \u001b[0;36mprocess_sampling_size\u001b[1;34m(n, frac, replace)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease enter a value for `frac` OR `n`, not both\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     98\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA negative number of rows requested. Please provide `n` >= 0.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m         )\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Load the full grocery customer dataset and take a random sample of 500 instances\n",
    "data = pd.read_csv('customer_personality.csv').sample(np.random.rand(500, 2)) # your code here , random_state=123)\n",
    "\n",
    "# Use StandardScaler() to standardize input features\n",
    "X = data[['Fruits', 'Meats']]\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "# Apply DBSCAN with epsilon=1 and min_samples = 8   \n",
    "dbscan = DBSCAN(eps=1, min_samples=8) # Your code here \n",
    "dbscan = dbscan.fit(X)\n",
    "\n",
    "# Print the cluster labels and core point indices\n",
    "labels = dbscan.labels_\n",
    "print('Labels:', labels) # Your code here\n",
    "print('Core points:', core_points = labels[labels != -1]) # Your code here\n",
    "print('Number of core points:', num_core_points = len(core_points)) # Your code here\n",
    "\n",
    "# Add the cluster labels to the dataset as strings\n",
    "data['clusters'] = dbscan.labels_.astype(str)\n",
    "\n",
    "# Sort by cluster label (for plotting purposes)\n",
    "data.sort_values(by='clusters', inplace=True)\n",
    "\n",
    "# Plot clusters on the original data\n",
    "p = sns.scatterplot(data=data, x='Fruits',\n",
    "                    y='Meats', hue='clusters',\n",
    "                    style='clusters')\n",
    "p.set_xlabel('Fruits', fontsize=16)\n",
    "p.set_ylabel('Meats', fontsize=16)\n",
    "p.legend(title='DBSCAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea545b6-c934-42ca-9f5a-be7bf69d22d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
